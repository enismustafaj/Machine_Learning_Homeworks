{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186573e9",
   "metadata": {},
   "source": [
    "**Programming Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387c6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f93186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.word_frequency = {\n",
    "            'make': 0,\n",
    "            'address': 0,\n",
    "            'all': 0,\n",
    "            '3d': 0,\n",
    "            'our': 0,\n",
    "            'over': 0,\n",
    "            'remove': 0,\n",
    "            'internet': 0,\n",
    "            'order': 0,\n",
    "            'mail': 0,\n",
    "            'receive': 0,\n",
    "            'will': 0,\n",
    "            'people': 0,\n",
    "            'report': 0,\n",
    "            'addresses': 0,\n",
    "            'free': 0,\n",
    "            'business': 0,\n",
    "            'email': 0,\n",
    "            'you': 0,\n",
    "            'credit': 0,\n",
    "            'your': 0,\n",
    "            'font': 0,\n",
    "            '000': 0,\n",
    "            'money': 0,\n",
    "            'hp': 0,\n",
    "            'hpl': 0,\n",
    "            'george': 0,\n",
    "            '650': 0,\n",
    "            'lab': 0,\n",
    "            'labs': 0,\n",
    "            'telnet': 0,\n",
    "            '857': 0,\n",
    "            'data': 0,\n",
    "            '415': 0,\n",
    "            '85': 0,\n",
    "            'technology': 0,\n",
    "            '1999': 0,\n",
    "            'parts': 0,\n",
    "            'pm': 0,\n",
    "            'direct': 0,\n",
    "            'cs': 0,\n",
    "            'meeting': 0,\n",
    "            'original': 0,\n",
    "            'project': 0,\n",
    "            're': 0,\n",
    "            'edu': 0,\n",
    "            'table': 0,\n",
    "            'conference': 0\n",
    "        }\n",
    "\n",
    "        self.character_frequency = {\n",
    "            ';': 0,\n",
    "            '(': 0,\n",
    "            '[': 0,\n",
    "            '!': 0,\n",
    "            '$': 0,\n",
    "            '#': 0\n",
    "        }\n",
    "\n",
    "        self.capital_length_average = 0\n",
    "        self.capital_length_longest = 0\n",
    "        self.capital_length_total = 0\n",
    "        self.text = ''\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as email:\n",
    "            text = email.read().splitlines()\n",
    "            self.text = ' '.join(text)\n",
    "\n",
    "    def extract(self):\n",
    "        for ch in self.text:\n",
    "            if ch in self.character_frequency.keys():\n",
    "                self.character_frequency[ch] += 1\n",
    "\n",
    "        clean_text = ''.join([ch if ch.isalnum() or ch == ' ' else ' ' for ch in self.text])\n",
    "\n",
    "        capital_sequence_count = 0\n",
    "\n",
    "        for word in clean_text.split(' '):\n",
    "            if word in self.word_frequency:\n",
    "                self.word_frequency[word] += 1\n",
    "\n",
    "            is_capital_sequence = False\n",
    "            capital_sequence_run_length = 0\n",
    "\n",
    "            for ch in word + ' ':\n",
    "                if ch.isupper():\n",
    "                    if is_capital_sequence:\n",
    "                        capital_sequence_run_length += 1\n",
    "                    else:\n",
    "                        is_capital_sequence = True\n",
    "                        capital_sequence_run_length = 1\n",
    "                else:\n",
    "                    if is_capital_sequence:\n",
    "                        is_capital_sequence = False\n",
    "                        capital_sequence_count += 1\n",
    "\n",
    "                        self.capital_length_longest = max(capital_sequence_run_length, self.capital_length_longest)\n",
    "                        self.capital_length_total += capital_sequence_run_length\n",
    "\n",
    "                        capital_sequence_run_length = 0\n",
    "\n",
    "        if capital_sequence_count > 0:\n",
    "            self.capital_length_average = self.capital_length_total / capital_sequence_count\n",
    "\n",
    "        new_word_map = dict([('word_freq_' + item[0], item[1]) for item in self.word_frequency.items()])\n",
    "        new_char_map = dict([('char_freq_' + item[0], item[1]) for item in self.character_frequency.items()])\n",
    "\n",
    "        new_word_map.update(new_char_map)\n",
    "        new_word_map[\"capital_run_length_average\"] = self.capital_length_average\n",
    "        new_word_map[\"capital_run_length_longest\"] = self.capital_length_longest\n",
    "        new_word_map[\"capital_run_length_total\"] = self.capital_length_total\n",
    "\n",
    "        return new_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d093ce90",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './file.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m featureExtractor \u001b[38;5;241m=\u001b[39m FeatureExtractor()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./file.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(featureExtractor\u001b[38;5;241m.\u001b[39mextract(), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mFeatureExtractor.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m email:\n\u001b[0;32m     73\u001b[0m         text \u001b[38;5;241m=\u001b[39m email\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './file.txt'"
     ]
    }
   ],
   "source": [
    "featureExtractor = FeatureExtractor()\n",
    "featureExtractor.load(\"./file.txt\")\n",
    "print(json.dumps(featureExtractor.extract(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
