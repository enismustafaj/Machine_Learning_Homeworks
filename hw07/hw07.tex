\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{April 5, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{7}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
In this task, we consider the training data
\begin{equation*}
    \resizebox{\textwidth}{!} {
    $\mathcal{T} = \left \{ ((1, 7)^{\top}, 25), ((2, 5)^{\top}, 21), ((2, 6)^{\top}, 14), ((3, 3)^{\top}, 32), ((7, 1)^{\top}, 14), ((3, 1)^{\top}, 14), ((4, 2)^{\top}, 25), ((5, 4)^{\top}, 18), ((5, 6)^{\top}, 12) \right \}$
    }
\end{equation*}
\begin{enumerate}[a)]
    \item Build a predictor using kNN regression for $k = 6$ and evaluate the training error of the predictor.
    
    The predictor for kNN regression with $k=6$ is 
    \begin{equation*}
        f(x) = \frac{1}{6} \cdot \sum_{x_{i} \in N_{6}(x)} y_{i}
    \end{equation*}
    
    where $N_{6}(x)$ are the 6 nearest neighbours of point $x$. We compute the predicted values for each input data:
    
    \begin{table}[h!]
        \centering
        \begin{tabular}{c c c}
             $f(x_{1}) = \frac{25 + 14 + 21 + 12 + 32 + 18}{6} = 20.33$ &  $f(x_{2}) = \frac{21 + 14 + 25 + 12 + 32 + 18}{6} = 20.33$ & $f(x_{3}) = \frac{21 + 14 + 25 + 12 + 32 + 18}{6} = 20.33$\\[1em]
             $f(x_{4}) = \frac{21 + 14 + 25 + 14 + 32 + 18}{6} = 20.67$ & $f(x_{5}) = \frac{18 + 14 + 25 + 12 + 32 + 14}{6} = 19.17$ & $f(x_{6}) = \frac{21 + 14 + 25 + 18 + 32 + 14}{6} = 20.67$ \\[1em]
             $f(x_{7}) = \frac{21 + 14 + 25 + 14 + 32 + 18}{6} = 20.67$ & $f(x_{8}) = \frac{18 + 21 + 25 + 12 + 32 + 14}{6} = 20.33$ & $f(x_{9}) = \frac{21 + 12 + 25 + 18 + 32 + 14}{6} = 20.33$ \\[1em]
        \end{tabular}
    \end{table}
    Now we compute the training error by the formula:
    \begin{equation*}
        TE(f, \mathcal{T}) = \frac{1}{N} \cdot \sum_{i = 1}^{N}L(y_{i}, f(x_{i})) = \frac{1}{9} \cdot \sum_{i = 1}^{9} (y_{i} - f(x_{i}))^{2}
    \end{equation*}
    
    By substituting the values from above, we get $TE(f, \mathcal{T}) = 39.51$
    
    \item Build a predictor using linear regression by least squares and evaluate the training error of the linear model.
    
    Firstly, we need to find the coefficients of the model by the formula:
    \begin{equation*}
        \beta = (\mathcal{X}^{T} \cdot X)^{-1} \cdot X^{T} \cdot y
    \end{equation*}
    
    where
    
    \begin{equation*}
        \mathcal{X} = \begin{pmatrix}1 & 1 & 7 \\ 1 & 2 & 5 \\ 1 & 2 & 6 \\1 & 3 & 3 \\ 1 & 7 & 1 \\ 1 & 3 & 1 \\ 1 & 4 & 2 \\ 1 & 5 & 4 \\ 1 & 5 & 6 \end{pmatrix} \hspace{1cm}
        \mathcal{X}^{T} = \begin{pmatrix}1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 2 & 3 & 7 & 3 & 4 & 5 & 5 \\ 7 & 5 & 6 & 3 & 1 & 1 & 2 & 4 & 6\end{pmatrix} \hspace{1cm}
        y = \begin{pmatrix} 25 \\ 21 \\ 14 \\ 32 \\ 14 \\ 14 \\ 25 \\ 18 \\ 12 \end{pmatrix}
    \end{equation*}
    
    By solving the linear equation:
    
    
    \begin{equation*}
        \mathcal{X}^{T} \cdot \mathcal{X} \cdot \beta = \mathcal{X}^{T} \cdot y
    \end{equation*}
    we get the vector of coefficients    $\beta = \begin{pmatrix}\frac{55170}{1831}\\[1em] -\frac{7465}{3662}\\[1em] -\frac{1619}{1831}\end{pmatrix}$  and the predictor $f(x) = x^{T} \cdot \beta$
    
    Now, we calculate the the predicted values for each of our input data
    
    \begin{table}[h!]
        \centering
        \begin{tabular}{c c c}
             $f(x_{1}) = x_{1}^{T} \cdot \beta = 21.9$ &  $f(x_{2}) = x_{2}^{T} \cdot \beta = 21.6$ & $f(x_{3}) = x_{3}^{T} \cdot \beta = 19.7$\\[1em]
             $f(x_{4}) = x_{4}^{T} \cdot \beta = 21.4$ & $f(x_{5}) = x_{5}^{T} \cdot \beta = 15$ & $f(x_{6}) = x_{6}^{T} \cdot \beta = 23.1$ \\[1em]
             $f(x_{7}) = x_{7}^{T} \cdot \beta = 20.2$ & $f(x_{8}) = x_{8}^{T} \cdot \beta = 16.4$ & $f(x_{9}) = x_{9}^{T} \cdot \beta = 14.6$ \\[1em]
        \end{tabular}
    \end{table}
    
    Now we compute the training error by the formula:
    \begin{equation*}
        TE(f, \mathcal{T}) = \frac{1}{N} \cdot \sum_{i = 1}^{N}L(y_{i}, f(x_{i})) = \frac{1}{9} \cdot \sum_{i = 1}^{9} (y_{i} - f(x_{i}))^{2}
    \end{equation*}
    
    By substituting the values from above in the formula, we get $TE(f, \mathcal{T}) = 31.7$
    
\end{enumerate}
\end{homeworkProblem}
\begin{homeworkProblem}
In this task, we consider the data set
\begin{equation*}
    \mathcal{T} = \left \{ (-2, 4), (2, 4), (1, 1), (-1, 1), (0, 0), (3, 9) \right \}
\end{equation*}
\begin{enumerate}[a)]
    \item Evaluate the (expected) generalization error of the kNN regressor with $k = 2$ by $\mathcal{K}$-fold cross validation with $\mathcal{K} = 3$. (Do a deterministic, i.e. not randomized, splitting of the given data following the ordering of the samples.)
    Following Algorithm 10 from the Lecture Notes for estimating the generalization error using $\mathcal{K}$-fold cross validation:
    
    \begin{enumerate}[1.]
        \item Split $\mathcal{T}$ into equally sized sets $\mathcal{T}_{1}, \ldots, \mathcal{T}_{\mathcal{K}}$
        \item For $i \in \{ 1, \ldots, \mathcal{K} \}$:
        \begin{enumerate}[a)]
            \item $\mathcal{T}_{\text{train}} \gets \mathcal{T} \setminus \mathcal{T}_{i}$
            \item $f_{\mathcal{T}_{\text{train}}} \gets \text{train}(f, \mathcal{T}_{\text{train}})$
            \item $\varepsilon_{i} \gets \frac{1}{|\mathcal{T}_{i}|} \sum_{(\boldsymbol x, \boldsymbol y) \in \mathcal{T}_{i}} L(\boldsymbol y, f_{\mathcal{T}_{\text{train}}}(\boldsymbol x))$
        \end{enumerate}
        \item $\varepsilon \gets \frac{1}{\mathcal{K}} \sum_{i = 1}^{K} \varepsilon_{i}$
        \item Return $\varepsilon$
    \end{enumerate}
    
    The following splits are created deterministically while using $\mathcal{K}$-fold cross validation with $\mathcal{K} = 3$:
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Split} & \textbf{Training Set $\mathcal{T}_{\text{train}}$} & \textbf{Test Set $\mathcal{T}_{i}$} \\ \hline
    1 & $\{(1, 1), (-1, 1), (0, 0), (3, 9)\}$ & $\{(-2, 4), (2, 4)\}$                   \\ \hline
    2 & $\{(-2, 4), (2, 4), (0, 0), (3, 9)\}$ & $\{(1, 1), (-1, 1)\}$ \\ \hline
    3 & $\{(-2, 4), (2, 4), (1, 1), (-1, 1)\}$ & $\{(0, 0), (3, 9)\}$\\ \hline
    \end{tabular}
    \end{table}
    \begin{itemize}
        \item \textbf{Split 1:}
        \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Data Point} & \textbf{True} & \textbf{Predicted} \\ \hline
        $(-2)$ & $(4)$ & $(0.5)$ \\ \hline
        $(2)$ & $(4)$ & $(5)$ \\ \hline
        \end{tabular}
        \end{table}
        
        Mean squared error for the split is $\varepsilon_{1} = 6.625$.
        
        \item \textbf{Split 2:}
        \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Data Point} & \textbf{True} & \textbf{Predicted} \\ \hline
        $(1)$ & $(1)$ & $(2)$ \\ \hline
        $(-1)$ & $(1)$ & $(2)$ \\ \hline
        \end{tabular}
        \end{table}
        Mean squared error for the split is $\varepsilon_{2} = 1$.
        
        \item \textbf{Split 3:}
        \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Data Point} & \textbf{True} & \textbf{Predicted} \\ \hline
        $(0)$ & $(0)$ & $(1)$ \\ \hline
        $(3)$ & $(9)$ & $(2.5)$ \\ \hline
        \end{tabular}
        \end{table}
        Mean squared error for the split is $\varepsilon_{3} = 21.625$.
    \end{itemize}
    
    The estimation of the (expected) generalization error is $\varepsilon = 9.75$.
    \item Evaluate the (expected) generalization error of the linear model by leave-one-out cross validation. (It is fine to use the help of a computer to fit the individual linear models.)
    
    Following Algorithm 9 from the Lecture Notes for estimating the generalization error using Leave One Out cross validation:
    
    \begin{enumerate}[1.]
        \item For $i \in \{ 1, \ldots, N \}$:
        \begin{enumerate}[a)]
            \item $\mathcal{T}_{\text{train}} \gets \mathcal{T} \setminus \{ (\boldsymbol x_{i}, \boldsymbol y_{i}) \}$
            \item $f_{\mathcal{T}_{\text{train}}} \gets \text{train}(f, \mathcal{T}_{\text{train}})$
            \item $\varepsilon_{i} \gets L(\boldsymbol y_{i}, f_{\mathcal{T}_{\text{train}}}(\boldsymbol x_{i}))$
        \end{enumerate}
        \item $\varepsilon \gets \frac{1}{\mathcal{K}} \sum_{i = 1}^{N} \varepsilon_{i}$
        \item Return $\varepsilon$
    \end{enumerate}
    
    The following splits are created while using Leave One Out cross validation:
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Split} & \textbf{Training Set $\mathcal{T}_{\text{train}}$} & \textbf{Test Set $\mathcal{T}_{i}$} \\ \hline
    1 & $\{(2, 4), (1, 1), (-1, 1), (0, 0), (3, 9)\}$ & $\{(-2, 4)\}$                   \\ \hline
    2 & $\{(-2, 4), (1, 1), (-1, 1), (0, 0), (3, 9)\}$ & $\{(2, 4)\}$ \\ \hline
    3 & $\{(-2, 4), (2, 4), (-1, 1), (0, 0), (3, 9)\}$ & $\{(1, 1)\}$\\ \hline
    4 & $\{(-2, 4), (2, 4), (1, 1), (0, 0), (3, 9)\}$ & $\{(-1, 1)\}$\\ \hline
    5 & $\{(-2, 4), (2, 4), (1, 1), (-1, 1), (3, 9)\}$ & $\{(0, 0)\}$\\ \hline
    6 & $\{(-2, 4), (2, 4), (1, 1), (-1, 1), (0, 0)\}$ & $\{(3, 9)\}$\\ \hline
    \end{tabular}
    \end{table}
    \begin{itemize}
        \item \textbf{Split 1:}
        Linear model fit from the training set data has the prediction function:
        \begin{equation*}
            \hat{y} = 1 + 2 x
        \end{equation*}
        
        The true value of the data point $(-2)$ is $4$. Predicted value for the data point $(-2)$ is $-3$.
        
        Loss using mean squared error for the split is $\varepsilon_{1} = 49$.
        \item \textbf{Split 2:}
        Linear model fit from the training set data has the prediction function:
        \begin{equation*}
            \hat{y} = 2.78378378 + 1.08108108 x
        \end{equation*}
        
        The true value of the data point $(2)$ is $4$. Predicted value for the data point $(2)$ is $4.94594595$.
        
        Loss using mean squared error for the split is $\varepsilon_{2} = 0.89481373$.
        \item \textbf{Split 3:}
        Linear model fit from the training set data has the prediction function:
        \begin{equation*}
            \hat{y} = 3.1627907 + 1.09302326 x
        \end{equation*}
        
        The true value of the data point $(1)$ is $1$. Predicted value for the data point $(1)$ is $4.25581395$.
        
        Loss using mean squared error for the split is $\varepsilon_{3} = 10.6003245$.
        \item \textbf{Split 4:}
        Linear model fit from the training set data has the prediction function:
        \begin{equation*}
            \hat{y} = 2.86486486 + 0.91891892 x
        \end{equation*}
        
        The true value of the data point $(-1)$ is $1$. Predicted value for the data point $(-1)$ is $1.94594595$.
        
        Loss using mean squared error for the split is $\varepsilon_{4} = 0.89481373$.
        \item \textbf{Split 5:}
        Linear model fit from the training set data has the prediction function:
        \begin{equation*}
            \hat{y} = 3.25581395 + 0.90697674 x
        \end{equation*}
        
        The true value of the data point $(0)$ is $0$. Predicted value for the data point $(0)$ is $3.25581395$.
        
        Loss using mean squared error for the split is $\varepsilon_{5} = 10.6003245$.
        \item \textbf{Split 6:}
        Linear model fit from the training set data has the prediction function:
        \begin{equation*}
            \hat{y} = 0 + 2 x
        \end{equation*}
        
        The true value of the data point $(3)$ is $9$. Predicted value for the data point $(3)$ is $2$.
        
        Loss using mean squared error for the split is $\varepsilon_{6} = 49$.
    \end{itemize}
    
    The estimation of the (expected) generalization error is $\varepsilon = 20.165046077460385$.
\end{enumerate}
\end{homeworkProblem}
\begin{homeworkProblem}
Provide quantitative reasoning to the questions below.
\begin{enumerate}[a)]
    \item What is the training error of kNN regression with neighbourhood size $k = 1$? Give the result and an explanation of how you get to the result.
    
    The training error of kNN regression with neighbourhood size $k = 1$ is $0$. In case of $k = 0$, the formula of kNN regression becomes:
    \begin{equation*}
        f(x) = \frac{1}{1} \cdot \sum_{x_{i} \in N_{1}(x)} y_{i} = \sum_{x_{i} \in N_{1}(x)} y_{i}
    \end{equation*}
    where $N_{1}(x)$ is the nearest neighbour of the point $x$. In this case the nearest neighbour of the point $x$ is the point itself, so the result of the predictor will be the output of the input data. So, the formula of kNN regression will be:
    \begin{equation*}
        f(x) = y
    \end{equation*}
    
    By substituting this on the training error formula we will have:
    \begin{equation*}
        TE(f, \mathcal{T}) = \frac{1}{N} \cdot \sum_{i = 1}^{N}L(y_{i}, f(x_{i})) = \frac{1}{9} \cdot \sum_{i = 1}^{9} (y_{i} -y_{i})^{2} = 0
    \end{equation*}
    
    
    \item What is the training error of linear regression with an output dimension of $K = 1$ for $N = 2$ distinct training samples? Give the result and an explanation of how you get to the result.
    
    The training error of linear regression with an output dimension of $K = 1$ for $N = 2$ distinct training samples is $0$, because there exists a line which connects $2$ different points. Let's take a set of $2$ points $\{(x_{1}, y_{1}), (x_{2}, y_{2})\}$ and try to find the vector of coefficients by the least squares formula:
    
    \begin{equation*}
         \beta = (\mathcal{X}^{T} \cdot X)^{-1} \cdot X^{T} \cdot y
    \end{equation*}
    
    where
    
    \begin{equation*}
        \mathcal{X} = \begin{pmatrix}1 & x_{1} \\ 1 & x_{2} \end{pmatrix} \hspace{1cm}
        \mathcal{X}^{T} = \begin{pmatrix}1 & 1 \\ x_{1} & x_{2} \end{pmatrix} \hspace{1cm}
        y = \begin{pmatrix} y_{1} \\ y_{2} \end{pmatrix}
    \end{equation*}
    
    By substituting in the formula we get:
    
    \begin{equation*}
        \beta = \begin{pmatrix}2 & x_{1} + x_{2} \\ x_{1} + X_{2} & x_{1}^{2} + x_{2}^{2} \end{pmatrix}^{-1} \cdot \begin{pmatrix}y_{1} + y_{2} \\ y_{1} \cdot x_{1} + y_{2} \cdot x_{2} \end{pmatrix}
    \end{equation*}
    
    Since we have only $2$ data points $\mathcal{X}^{T} \cdot \mathcal{X}$ will always produce a $2x2$ matrix and its inverse can easily be found
    
    \begin{equation*}
        \beta = \frac{1}{(x_{1} - x_{2})^{2}} \cdot \begin{pmatrix}x_{1}^{2} + x_{2}^{2} & - x_{1} - x_{2} \\ - x_{1} - X_{2} & 2 \end{pmatrix} \cdot \begin{pmatrix}y_{1} + y_{2} \\ y_{1} \cdot x_{1} + y_{2} \cdot x_{2} \end{pmatrix}
    \end{equation*}
    
    \begin{equation*}
        \beta = \frac{1}{(x_{1} - x_{2})^{2}} \cdot \begin{pmatrix}(y_{2} \cdot x_{1} - y_{1} \cdot x_{2}) \cdot (x_{1} - x_{2}) \\ (y_{1} - y_{2}) \cdot (x_{1} - x_{2}) \end{pmatrix}
    \end{equation*}
    
    \begin{equation*}
        \beta = \begin{pmatrix}\frac{y_{2} \cdot x_{1} - y_{1} \cdot x_{2}}{x_{1} - x_{2}} \\[1em] \frac{y_{1} - y_{2}}{x_{1} - x_{2}} \end{pmatrix}
    \end{equation*}
    
    By inserting each input data point in the formula:
    \begin{equation*}
        f(x) = \frac{y_{2} \cdot x_{1} - y_{1} \cdot x_{2}}{x_{1} - x_{2}} + \frac{y_{1} - y_{2}}{x_{1} - x_{2}} \cdot x
    \end{equation*}
    
    we get the same output as the initial data point. So, the training error formula becomes:
    \begin{equation*}
        TE(f, \mathcal{T}) = \frac{1}{N} \cdot \sum_{i = 1}^{N}L(y_{i}, f(x_{i})) = \frac{1}{2} \cdot \sum_{i = 1}^{2} (y_{i} -y_{i})^{2} = 0
    \end{equation*}
    
\end{enumerate}    
\end{homeworkProblem}
\begin{programmingProblem}
In this programming exercise, you will implement the validation set approach and $\mathcal{K}$-fold cross validation, while recalling that leave-one-out cross validation is $\mathcal{K}$-fold cross validation for $\mathcal{K} = N$, if $N$ is the number of training samples.\\
Implement the study carried out in Example 6.3 from the lecture notes. Note that due to randomization, you might get other results that the ones shown in the example.
\end{programmingProblem}

\end{document}
