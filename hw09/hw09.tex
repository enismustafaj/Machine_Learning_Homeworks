\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{April 19, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{9}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
We would like to solve a simple classification problem using paper and pencil. To this end, you are given the training data
\begin{equation*}
    \mathcal{T} = \{ (0.3, 1), (1.8, 1), (1.5, 1), (4.8, 2), (2.6, 2) \}
\end{equation*}

The objective is to predict the class labels for the two evaluation points $x_{1} = 2.4$ and $x_{2} = 6.2$. Use linear regression on the indicator matrix to build the necessary predictor and evaluate it at $x_{1}$ and $x_{2}$. In addition, evaluate the \textit{training error} using the 0-1 loss. \\

Firstly, we build the matrices $\mathcal{X}$ and $\mathcal{Y}$:

\begin{equation*}
    \mathcal{X} = \begin{pmatrix} 1 & 0.3 \\ 1 & 1.8 \\ 1 & 1.5 \\ 1 & 4.8 \\ 1 & 2.6 \end{pmatrix} \hspace{1cm}
    \mathcal{Y} = \begin{pmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \end{pmatrix} \hspace{1cm}
    \mathcal{X}^{T} = \begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\ 0.3 & 1.8 & 1.5 & 4.8  & 2.6 \end{pmatrix} 
\end{equation*}

By using the formula of the least squares, we have:

\begin{equation*}
    \beta = (\mathcal{X}^{T} \cdot \mathcal{X})^{-1} \cdot \mathcal{X}^{T} \cdot \mathcal{Y}
\end{equation*}

\begin{equation*}
    \beta = \begin{pmatrix}5 & 11\\ 11 & 35.38\end{pmatrix}^{-1} \cdot \begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\ 0.3 & 1.8 & 1.5 & 4.8  & 2.6 \end{pmatrix} \cdot \begin{pmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix}  1.19033989 & -0.19033989 \\ -0.26833631 &  0.26833631\end{pmatrix}
\end{equation*}

Now, we find the class that the input data belongs to by choosing the largest output between the entries of the result vector:

\begin{itemize}
    \item $x_{1} = 2.4$
        \begin{equation*}
            y = \begin{pmatrix} 1 & x^{T} \end{pmatrix} \cdot \beta = \begin{pmatrix}1 & 2.4 \end{pmatrix} \cdot \begin{pmatrix}  1.19033989 & -0.19033989 \\ -0.26833631 &  0.26833631\end{pmatrix} = \begin{pmatrix}0.54633274 \\ 0.45366726\end{pmatrix} 
        \end{equation*}
        So, the input data belongs to the first class $(2.4, 1)$
        
    \item $x_{2} = 6.2$
        \begin{equation*}
            y = \begin{pmatrix} 1 & x^{T} \end{pmatrix} \cdot \beta = \begin{pmatrix}1 & 6.2 \end{pmatrix} \cdot \begin{pmatrix}  1.19033989 & -0.19033989 \\ -0.26833631 &  0.26833631\end{pmatrix} = \begin{pmatrix}-0.47334526 \\ 1.47334526\end{pmatrix} 
        \end{equation*}
        So, the input data belongs to the second class $(6.2, 2)$
\end{itemize}

Now, we calculate the training error by using the 0-1 loss, where 0-1 loss is given by:
\begin{equation*}
    L_{0-1}(g, g') =  \begin{cases} 
      0 & g = g' \\
      1 & g \neq g'  
   \end{cases}
\end{equation*}

\begin{equation*}
    TE = \sum_{i = 1}^{5}L_{0-1}(y_{1}, argmax((1, x^{T}) \cdot \beta))
\end{equation*}

\begin{equation*}
    \begin{split}
        TE = L_{0-1}(1, argmax\begin{pmatrix}1.109839 \\ -0.109839\end{pmatrix}) + L_{0-1}(1, argmax\begin{pmatrix}0.70733453 \\ 0.29266547\end{pmatrix}) + L_{0-1}(1, argmax\begin{pmatrix}0.78783542 \\ 0.21216458\end{pmatrix}) \\  + L_{0-1}(2, argmax\begin{pmatrix}-0.09767442 \\  1.09767442\end{pmatrix}) + L_{0-1}(2, argmax\begin{pmatrix}0.49266547 \\ 0.50733453\end{pmatrix}) = 0
    \end{split}
\end{equation*}

\end{homeworkProblem}

\begin{homeworkProblem}
We again start from the training data set
\begin{equation*}
    \mathcal{T}_{train} = \{ (-2.1, 1), (-0.9, 1), (0.6, 2), (1.5, 2), (2.7, 2) \}
\end{equation*}

for a paper and pencil classification task. In addition, you are given the validation set
\begin{equation*}
    \mathcal{T}_{val} = \{ (-1.2, 1), (0.5, 1), (1.4, 2) \}
\end{equation*}
\begin{enumerate}[a)]
    \item Use linear discriminant analysis to build a classifier based on the training data.
        \begin{itemize}
            \item Firstly, we find the number of members for each class: $N_{g}$ = \{2, 3\}.
            \item Then we find the probability set for each of the classes: $p(g) = \{\frac{2}{5}, \frac{3}{5}\}$ = \{0.4, 0.6\}
            \item We find the set of mean values of the input data for each class:
            \begin{equation*}
                \mu_{g} = \{\frac{-2.1 - 0.9}{2}, \frac{0.6 + 1.5 + 2.7}{3}\} = \{-1.5, 1.6\}
            \end{equation*}
            \item Now, we find the covariance matrix by the formula:
            \begin{equation*}
                \Sigma = \sum_{g = 1}^{r}\sum_{(x, g) \in \mathcal{T}}(x - \mu_{g}) \cdot (x - \mu_{g})^{T} / (N - r)
            \end{equation*}
            Since the input data is one dimensional, the covariance matrix will be one dimensional as well.
            \begin{equation*}
            \begin{split}
                \Sigma = \frac{(-2.1 + 1.5)^{2}}{3} + \frac{(-0.9 + 1.5)^{2}}{3}   + \frac{(-2.1 - 0.6)^{2}}{3} + \\ \frac{(0.6 - 1.6)^{2}}{3}  +\frac{(1.5 - 1.6)^{2}}{3} + \frac{(2.7 - 1.6)^{2}}{3} = 0.98
            \end{split}
            \end{equation*}
        \end{itemize}
        
        The produced classifier is expressed as:
        \begin{equation*}
            g = argmax \Bigg(\begin{pmatrix}-0.91629073 \\ -0.51082562\end{pmatrix} + x \cdot \frac{1}{0.98} \cdot \begin{pmatrix}0.4 \\ 0.6\end{pmatrix} - \frac{1}{2} \cdot \begin{pmatrix}0.4 & 0.6\end{pmatrix} \cdot \frac{1}{0.98} \cdot \begin{pmatrix}0.4 \\ 0.6\end{pmatrix}\Bigg) 
        \end{equation*}
        
        
    \item Evaluate the generalization error for the just constructed predictor using the 0-1 loss and the validation set approach.
    
    We evaluate the resulting class for each input data in the evaluation set:
    
    \begin{itemize}
        \item $x_{1}$ = -1.5, 
            $g = argmax\begin{pmatrix}-0.22751522 \\ -3.77613175\end{pmatrix} = 1$
        \item $x_{2}$ = 0.5, 
            $g = argmax\begin{pmatrix}-2.82955604 \\ -1.00062154\end{pmatrix} = 2$
        \item $x_{3}$ = 1.4, 
            $g = argmax\begin{pmatrix}-4.20710706 \\ 0.46876621\end{pmatrix} = 1$
    \end{itemize}
    
    From the results above, we see that the class predicted for the second input data is different from the initial class. Thus, the 0-1 loss will be $1$ the generalization error is: $GE = 1$
\end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
Prove Lemma 8.2 from the lecture.\\

\textbf{Lemma 8.2} - With the setting from Theorem 8.4, the gradient of the functional $J(\mathcal{B})$ with respect to a fixed class label $g$, which we call "$\nabla_{g}$" is given by

\begin{equation*}
    \nabla_{g} J(\mathcal{B}) = \begin{pmatrix}\frac{\partial}{\partial \beta_{0}^{(g)}} J(\mathcal{B}) \\ \vdots \\ \frac{\partial}{\partial \beta_{D}^{(g)}} J(\mathcal{B}) \end{pmatrix}  = \frac{1}{N} \sum_{i = 1}^{N} (p_{\mathcal{B}} (g | \boldsymbol {x}_{i}) - p(g | \boldsymbol {x}_{i})) \begin{pmatrix} 1 \\ \boldsymbol x_{i} \end{pmatrix}
\end{equation*}

Based on the setting of Theorem 8.4 and indirectly Method 12, the following statements are known

\begin{equation*}
    s_{g}(\boldsymbol x) = \beta_{g}^{(g)} + \sum_{i = 1}^{D} \beta_{i}^{(g)} x_{i} \quad \quad \text{for } g = 1, \ldots, r
\end{equation*}
\begin{equation*}
    p_{\mathcal{B}}(g | \boldsymbol x) \approx \frac{\text{exp}(s_{g}(\boldsymbol x))}{\sum_{h = 1}^{r} \text{exp}(s_{h}(\boldsymbol x))}\quad \quad 
    \text{for } g = 1, \ldots, r
\end{equation*}
\end{homeworkProblem}

We start from the result of Theorem 8.4, and the goal is to minimize the cost functional with respect to a given class $g$. This minimization can be found by calculating the gradient and equating it to 0. Since the solution is not dependent on any scaling, we are free to add any scaling.

Therefore, the cost functional that we are minimizing is:
\begin{equation*}
    \begin{split}
        \frac{1}{N} J(\mathcal{B}) & = - \frac{1}{N} \sum_{i = 1}^{N} \left [ \left ( \sum_{g = 1}^{r} p(g | \boldsymbol x_{i}) s_{g}(\boldsymbol x_{i}) \right ) - \text{log} {\left ( \sum_{h = 1}^{r} \text{exp}(s_{h}(\boldsymbol x_{i}) \right )} \right ] \\
        & = - \frac{1}{N} \sum_{i = 1}^{N} \left ( p(1 | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ] + \ldots + p(r | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ] \right ) \\
        & \quad \quad \quad \quad \quad \quad \quad - \text{log} {\left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )}
    \end{split}
\end{equation*}

The gradient with respect to the coefficients $\mathcal{B}$, given a fixed class $g$, can be calculated as:
\begin{equation*}
    \nabla_{g} \frac{1}{N} J(\mathcal{B}) = \frac{1}{N} \begin{pmatrix}\frac{\partial}{\partial \beta_{0}^{(g)}} J(\mathcal{B}) \\ \vdots \\ \frac{\partial}{\partial \beta_{D}^{(g)}} J(\mathcal{B}) \end{pmatrix}
\end{equation*}

\begin{equation*}
    \begin{split}
        \frac{\partial}{\partial \beta_{0}^{(g)}} \frac{1}{N} J(\mathcal{B}) & = - \frac{\partial}{\partial \beta_{0}^{(g)}} \frac{1}{N} \sum_{i = 1}^{N} \left ( p(1 | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ] + \ldots + p(r | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ] \right ) \\
        & \quad \quad \quad \quad \quad \quad \quad \quad \quad - \text{log} {\left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )} \\
        & \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \vdots \\
        \frac{\partial}{\partial \beta_{D}^{(g)}} \frac{1}{N} J(\mathcal{B}) & = - \frac{\partial}{\partial \beta_{D}^{(g)}} \frac{1}{N} \sum_{i = 1}^{N} \left ( p(1 | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ] + \ldots + p(r | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ] \right ) \\
        & \quad \quad \quad \quad \quad \quad \quad \quad \quad - \text{log} {\left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )}
    \end{split}
\end{equation*}

Using the additive property, in this case, partial derivative of a sum is the sum of the partial derivatives. To avoid performing the following steps manually, we are taking the partial derivative of one of the coefficients, $\beta_{k}$, where $k \in \{0, \ldots, D\}$ to represent what we are doing for each partial derivative of the gradient.

\begin{equation*}
    \begin{split}
        \frac{\partial}{\partial \beta_{k}^{(g)}} \frac{1}{N} J(\mathcal{B}) & = - \frac{\partial}{\partial \beta_{k}^{(g)}} \frac{1}{N} \sum_{i = 1}^{N} \left ( p(1 | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ] + \ldots + p(r | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ] \right ) \\
        & \quad \quad \quad \quad \quad \quad \quad \quad \quad - \text{log}{\left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )} \\
        & = - \frac{1}{N} \sum_{i = 1}^{N} \frac{\partial}{\partial \beta_{k}^{(g)}} \left ( p(1 | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ] + \ldots + p(r | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ] \right ) \\ 
        & \quad \quad \quad \quad \quad \quad \quad \quad - \frac{\partial}{\partial \beta_{k}^{(g)}} \text{log}{\left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )}
    \end{split}
\end{equation*}

Considering only the first term of the sum:
\begin{equation*}
    \frac{\partial}{\partial \beta_{k}^{(g)}} \left ( p(1 | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ] + \ldots + p(r | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ] \right )
\end{equation*}

Taking the partial derivative only given the class $g$ will leave only one of the product terms in the sum.
\begin{equation*}
    \frac{\partial}{\partial \beta_{k}^{(g)}} \left ( p(g | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ] \right )
\end{equation*}

Continuing the derivation with respect to the coefficient $\beta_{k}$:
\begin{equation*}
    \frac{\partial}{\partial \beta_{k}^{(g)}} \left ( p(g | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ] \right ) = \begin{cases} p(g | \boldsymbol x_{i}) \cdot 1 & \text{if } k = 0 \\ p(g | \boldsymbol x_{i}) \cdot x_{ik} & \text{otherwise} \end{cases} \quad \text{for } k \in \{0, \ldots, D \}
\end{equation*}

Condensing the result to find the gradient of only the first term of them sum given the class $g$ with respect to the set of coefficients:
\begin{equation*}
    \begin{pmatrix}
    \frac{\partial}{\partial \beta_{0}^{(g)}} \left ( p(g | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ] \right ) \\
    \vdots \\
    \frac{\partial}{\partial \beta_{D}^{(g)}} \left ( p(g | \boldsymbol x_{i}) \cdot \left [ \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ] \right )
    \end{pmatrix} = \begin{pmatrix}
    p(g | \boldsymbol x_{i}) \\ \vdots \\ p(g | \boldsymbol x_{i}) \cdot x_{iD}
    \end{pmatrix} = p(g | \boldsymbol x_{i}) \cdot \begin{pmatrix}
    1 \\ \boldsymbol x_{i}
    \end{pmatrix}
\end{equation*}

Considering only the second term of the sum:
\begin{equation*}
    \frac{\partial}{\partial \beta_{k}^{(g)}} \text{log}{\left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )}
\end{equation*}

Letting $z$ be equal to the sum of exponentials inside of the logarithm, and continuing the derivation:
\begin{equation*}
    \begin{split}
        \frac{\partial}{\partial \beta_{k}^{(g)}} \text{log}{(z)} & = \frac{1}{z} \frac{\partial}{\partial \beta_{k}^{(g)}} \left ( \text{exp}\left ( \beta_{0}^{(1)} + \sum_{j = 1}^{D} \beta_{j}^{(1)} \boldsymbol x_{ij} \right ) + \ldots + \text{exp}\left ( \beta_{0}^{(r)} + \sum_{j = 1}^{D} \beta_{j}^{(r)} \boldsymbol x_{ij} \right ) \right )
    \end{split}
\end{equation*}

Taking the partial derivative only given the class $g$ will leave only one exponential term in the sum.
\begin{equation*}
    \frac{1}{z} \frac{\partial}{\partial \beta_{k}^{(g)}} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right )
\end{equation*}

Continuing the derivation with respect to the coefficient $\beta_{k}$:
\begin{equation*}
    \frac{1}{z} \frac{\partial}{\partial \beta_{k}^{(g)}} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ) = \begin{cases} \frac{1}{z} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ) \cdot 1 & \text{if } k = 0 \\ \frac{1}{z} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ) \cdot x_{ik} & \text{otherwise} \end{cases} \quad \text{for } k \in \{0, \ldots, D \}
\end{equation*}

From the setting, we recall that:
\begin{equation*}
    p_{\mathcal{B}}(g | \boldsymbol x) \approx \frac{\text{exp}(s_{g}(\boldsymbol x))}{\sum_{h = 1}^{r} \text{exp}(s_{h}(\boldsymbol x))} = \frac{1}{z} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right )
\end{equation*}

Condensing the result to find the gradient of only the second term of them sum given the class $g$ with respect to the set of coefficients:
\begin{equation*}
    \begin{pmatrix}
    \frac{\partial}{\partial \beta_{0}^{(g)}} \left ( \frac{1}{z} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ) \right ) \\
    \vdots \\
    \frac{\partial}{\partial \beta_{D}^{(g)}} \left ( \frac{1}{z} \text{ exp}\left ( \beta_{0}^{(g)} + \sum_{j = 1}^{D} \beta_{j}^{(g)} \boldsymbol x_{ij} \right ) \right )
    \end{pmatrix} = \begin{pmatrix}
    p_{\mathcal{B}}(g | \boldsymbol x_{i}) \\ \vdots \\ p_{\mathcal{B}}(g | \boldsymbol x_{i}) \cdot x_{iD}
    \end{pmatrix} = p_{\mathcal{B}}(g | \boldsymbol x_{i}) \cdot \begin{pmatrix}
    1 \\ \boldsymbol x_{i}
    \end{pmatrix}
\end{equation*}

Combining both results:
\begin{equation*}
    \begin{split}
        \nabla_{g} \frac{1}{N} J(\mathcal{B}) = \frac{1}{N} \begin{pmatrix}\frac{\partial}{\partial \beta_{0}^{(g)}} J(\mathcal{B}) \\ \vdots \\ \frac{\partial}{\partial \beta_{D}^{(g)}} J(\mathcal{B}) \end{pmatrix} & = \frac{1}{N} \begin{pmatrix} 
        - \sum_{i = 1}^{N} p(g | \boldsymbol x_{i}) - p_{\mathcal{B}}(g | \boldsymbol x_{i}) \\
        \vdots \\
        - \sum_{i = 1}^{N} p(g | \boldsymbol x_{i}) \cdot x_{iD} - p_{\mathcal{B}}(g | \boldsymbol x_{i}) \cdot x_{iD}
        \end{pmatrix} \\
        & = - \frac{1}{N} \begin{pmatrix} 
        \sum_{i = 1}^{N} (p(g | \boldsymbol x_{i}) - p_{\mathcal{B}}(g | \boldsymbol x_{i})) \cdot 1 \\
        \vdots \\
        \sum_{i = 1}^{N} (p(g | \boldsymbol x_{i}) - p_{\mathcal{B}}(g | \boldsymbol x_{i})) \cdot x_{iD}
        \end{pmatrix} \\
        & = \frac{1}{N} \sum_{i = 1}^{N} (p_{\mathcal{B}}(g | \boldsymbol x_{i}) - p(g | \boldsymbol x_{i})) \begin{pmatrix} 1 \\ \boldsymbol x_{i} \end{pmatrix}
    \end{split}
\end{equation*}
\begin{programmingProblem}
\begin{enumerate}[a)]
    \item Start from Example 8.1 from the lecture notes for which you have the Python source code available as Jupyter notebook. Ignore kNN classification and linear regression on indicator matrix and (only) re-implement the linear discriminant analysis by yourself. Verify the correctness of your implementation by cross-checking it with Example 8.1.
    
    The implementation of linear discriminant analysis can be found in the file \texttt{programming\_exercises.ipynb}. After the implementation, the comparison is done and an accuracy of our implementation based on the results of Example 8.1 is given.
    \item Now, we would like to compare the performance of the just implemented linear discriminant analysis to the performance of kNN classification (based e.g. on Scikit-learn) on SPAM data. Use as data set the \href{https://archive.ics.uci.edu/ml/datasets/spambase}{Spambase Data Set} from the UCI Machine Learning Repository. To carry out the comparison, implement the validation set approach with the 0-1 loss. Randomly split the data set into $N_{train} = 1000$ training samples and $N_{val} = 100$ validation samples and use the same split to evaluate the generalization error for LDA and kNN (with $k = 3$) classification.
    
    The solution to this part of the exercise can be found in the file \texttt{programming\_exercises.ipynb}. The validation set approach is based on the split retrieved by \texttt{train\_test\_split} function. The generalization error based on the validation set approach is calculated using the function \texttt{zero\_one\_loss}, whose implementation is just a simple loop going over each pair of outputs, real and predicted, and summing up the cases when the predicted output is not the same as the test set one. In the end, the result is divided by the size of the test split.
\end{enumerate}
\end{programmingProblem}

\end{document}