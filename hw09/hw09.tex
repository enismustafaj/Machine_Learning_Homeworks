\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{April 19, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{9}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
We would like to solve a simple classification problem using paper and pencil. To this end, you are given the training data
\begin{equation*}
    \mathcal{T} = \{ (0.3, 1), (1.8, 1), (1.5, 1), (4.8, 2), (2.6, 2) \}
\end{equation*}

The objective is to predict the class labels for the two evaluation points $x_{1} = 2.4$ and $x_{2} = 6.2$. Use linear regression on the indicator matrix to build the necessary predictor and evaluate it at $x_{1}$ and $x_{2}$. In addition, evaluate the \textit{training error} using the 0-1 loss.
\end{homeworkProblem}

\begin{homeworkProblem}
We again start from the training data set
\begin{equation*}
    \mathcal{T}_{train} = \{ (-2.1, 1), (-0.9, 1), (0.6, 2), (1.5, 2), (2.7, 2) \}
\end{equation*}

for a paper and pencil classification task. In addition, you are given the validation set
\begin{equation*}
    \mathcal{T}_{val} = \{ (-1.2, 1), (0.5, 1), (1.4, 2) \}
\end{equation*}
\begin{enumerate}[a)]
    \item Use linear discriminant analysis to build a classifier based on the training data.
    \item Evaluate the generalization error for the just constructed predictor using the 0-1 loss and the validation set approach.
\end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
Prove Lemma 8.2 from the lecture.\\

\textbf{Lemma 8.2} - With the setting from Theorem 8.4, the gradient of the functional $J(\mathcal{B})$ with respect to a fixed class label $g$, which we call "$\nabla_{g}$" is given by

\begin{equation*}
    \nabla_{g} J(\mathcal{B}) = \begin{pmatrix}\frac{\partial}{\partial \beta_{0}^{(g)}} J(\mathcal{B}) \\ \vdots \\ \frac{\partial}{\partial \beta_{D}^{(g)}} J(\mathcal{B}) \end{pmatrix}  = \frac{1}{N} \sum_{i = 1}^{N} (p_{\mathcal{B}} (g | \boldsymbol {x}_{i}) - p(g | \boldsymbol {x}_{i})) \begin{pmatrix} 1 \\ \boldsymbol x_{i} \end{pmatrix}
\end{equation*}

Based on the setting of Theorem 8.4 and indirectly Method 12, the 
\end{homeworkProblem}

\begin{programmingProblem}
\begin{enumerate}[a)]
    \item Start from Example 8.1 from the lecture notes for which you have the Python source code available as Jupyter notebook. Ignore kNN classification and linear regression on indicator matrix and (only) re-implement the linear discriminant analysis by yourself. Verify the correctness of your implementation by cross-checking it with Example 8.1.
    \item Now, we would like to compare the performance of the just implemented linear discriminant analysis to the performance of kNN classification (based e.g. on Scikit-learn) on SPAM data. Use as data set the \href{https://archive.ics.uci.edu/ml/datasets/spambase}{Spambase Data Set} from the UCI Machine Learning Repository. To carry out the comparison, implement the validation set approach with the 0-1 loss. Randomly split the data set into $N_{train} = 1000$ training samples and $N_{val} = 100$ validation samples and use the same split to evaluate the generalization error for LDA and kNN (with $k = 3$) classification.
\end{enumerate}
\end{programmingProblem}

\end{document}