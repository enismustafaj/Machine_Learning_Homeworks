\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{March 15, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{5}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
In this problem, you will need to remind yourself about the definition of a positive definite matrix. Moreover you will look for local maxima and minima of a function of two variables.

\begin{enumerate}[a)]
    \item Compute for the following matrices, whether they are positive definite.
    \begin{equation*}
        M = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}, \quad N = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}
    \end{equation*}
    \item You are given the following function:
    \begin{equation*}
        f(x, y) = 3 x^{2} y + y^{3} - 3 x^{2} - 3 y^{2} + 2 
    \end{equation*}
    Use the Knowledge 4.1 statement from the lecture notes to find and classify all locations of minima and maxima of this function.
\end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
You are given the training data $\mathcal{T} = \left \{ ((1, 1)^{\top}, 2), ((1, 2)^{\top}, 3), ((2, 2)^{\top}, 3), ((2, 4)^{\top}, 4) \right \}$.
\begin{enumerate}[a)]
    \item Use linear regression by least squares to compute a predictor for the output. Do this by hand, i.e., don't use the computer and show all computation steps.
    \item Predict the output for $\mathbf{x} = (1.5, 1.5)^{\top}$.
\end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
Prove Theorem 3.2 from the lecture. If you consider this as too hard, consult the literature from this field to solve the problem. In case you use external literature, make sure that you give a citation for that literature (i.e. say where you took it from) and rephrase the proof appropriately.\\

Assuming the input, output and mixed joint density from Theorem 3.2. To find the function $f : R^{D} \to R_{G}$ that minimizes the expected prediction error EPE($f$) with respect to the 0 - 1 loss, the following steps can be taken:
\begin{equation*}
    \begin{split}
        \argmin_{g \in R_{G}} \text{EPE}(f(\textbf{x})) & = \argmin_{g \in R_{G}} E[L(g, f(\textbf{x})) | \textbf{X} = \textbf{x}] \\
        & = \argmin_{g \in R_{G}} P(g = g_{1} | \textbf{X} = \textbf{x}) \cdot L(g_{1}, f(\textbf{x})) + \ldots + P(g = g_{r} | \textbf{X} = \textbf{x}) \cdot L(g_{r}, f(\textbf{x})) \\
        & = \argmin_{g \in R_{G}} \sum_{i = 1}^{r} P(g = g_{i} | \textbf{X} = \textbf{x}) \cdot L(g_{i}, f(\textbf{x})
    \end{split}
\end{equation*}

Due to the loss function having a range of 0 and 1, the last result can be simplified to a sum of the probabilities for which the predicted class, $f(x)$, does not equal the true class, $g$.
\begin{equation*}
    \begin{split}
        \argmin_{g \in R_{G}} \text{EPE}(f(\textbf{x})) & = \argmin_{g \in R_{G}} P(g \neq f(\textbf{x}) | \textbf{X} = \textbf{x}) \\
        & = \argmin_{g \in R_{G}} 1 - P(g = f(\textbf{x}) | \textbf{X} = \textbf{x}) \\
        & = \argmax_{g \in R_{G}} P(g = f(\textbf{x}) | \textbf{X} = \textbf{x})
    \end{split}
\end{equation*}

If $\mathfrak{g}$ is the true output class, such that $p(\mathfrak{g} | \textbf{x}) = \max_{g \in R_{G}} p(g | \textbf{x})$, then the minimized $f(\textbf{x})$ found above matches the definition of the Bayes classifier.

\end{homeworkProblem}

\begin{programmingProblem}
Consider the Examples 4.2 and 4.3 from the lecture, for which you also have access to the source code. Complete the following tasks:
\begin{enumerate}[a)]
    \item (Re-)implement Example 4.3. This time, however you need to implement the linear regression yourself, without a machine learning library. (If you implement in Python, just start from the available Jupyter notebook.) Verify the correctness of your implementation by cross-checking it with Example 4.3.
    \item Apply your implementation to the Energy efficiency Data Set from the UCI Machine Learning Repository. Build the predictor for the required heating load on the full data set and predict the load on the first three samples of the data set.
    \item Implement Example 4.5 starting from Example 4.2 from the lecture notes.
\end{enumerate}

The implementation of the linear regressor by least squares can be found in the file \texttt{programming\_exercises.ipynb}. The class offers two methods, \texttt{fit} and \texttt{predict}, which add the data and train the model, and predict the expected regression value.
\end{programmingProblem}

\end{document}
