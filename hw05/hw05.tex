\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{March 15, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{5}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
In this problem, you will need to remind yourself about the definition of a positive definite matrix. Moreover you will look for local maxima and minima of a function of two variables.

\begin{enumerate}[a)]
    \item Compute for the following matrices, whether they are positive definite.
    \begin{equation*}
        M = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}, \quad N = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}
    \end{equation*}
    
    To compute if a matrix is positive definite, we compute the eigenvalues of the matrix and see if they are positive.
    \begin{enumerate}[a)]
        \item Eigenvalues of the first matrix
            \begin{equation*}
                M - \lambda \cdot I = \begin{bmatrix} 1 - \lambda & 2 \\ 2 & 1 - \lambda \end{bmatrix}
            \end{equation*}
            
            To calculate the eigenvalues we find the roots of the determinant.
            \begin{equation*}
                \Delta = (1 - \lambda)^{2} - 4 = 0
            \end{equation*}
            From the equation above we find that the eigenvalues are $-1$ and $-3$, so the matrix is not positive definite.
            
        \item Eigenvalues of the second matrix
            For the second matrix we repeat the steps from above.
                \begin{equation*}
                    N - \lambda \cdot I = \begin{bmatrix} 2 - \lambda & -1 & 0 \\ -1 & 2 - \lambda & -1 \\ 0 & -1 & 2 - \lambda \end{bmatrix}
                \end{equation*}
                \begin{equation*}
                  \Delta = (2 - \lambda) \cdot [(2 - \lambda)^{2} - 1] - (\lambda - 2) = 0
                \end{equation*}
                \begin{equation*}
                  \Delta = (2 - \lambda) \cdot [(2 - \lambda)^{2} - 2] = 0
                \end{equation*}
                
                From the equation above, the eigenvalues are $2$, $2 - \sqrt{2}$, $2 + \sqrt{2}$. The eigenvalues are positive, so the matrix is positive definite.
        
         
    \end{enumerate}
    \item You are given the following function:
    \begin{equation*}
        f(x, y) = 3 x^{2} y + y^{3} - 3 x^{2} - 3 y^{2} + 2 
    \end{equation*}
    Use the Knowledge 4.1 statement from the lecture notes to find and classify all locations of minima and maxima of this function.
    
    The gradient of the function $f(x, y)$ is
    
    \begin{equation*}
        \nabla f = \begin{pmatrix} \dfrac{\partial f(x, y)}{\partial x} \\[1em] \dfrac{\partial f(x, y)}{\partial y} \end{pmatrix} = \begin{pmatrix} 6xy - 6x \\[1em] 3x^{2} + 3y^{2} - 6y \end{pmatrix}
    \end{equation*}
    
    The points for which the gradient is equal to $\Vec{0}$ are $\{(1, 1), (-1, 1), (0, 2), (0, 0)\}$. The Hessian matrix of the function $f$ is
    
    \begin{equation*}
        \mathcal{H}_{f(x, y)} = \begin{pmatrix} \dfrac{\partial^{2} f(x, y)}{\partial x^{2}} &  \dfrac{\partial^{2} f(x, y)}{\partial x \cdot \partial y} \\[1em] \dfrac{\partial^{2} f(x, y)}{\partial y \cdot \partial x} &  \dfrac{\partial^{2} f(x, y)}{\partial y^{2}} \end{pmatrix} = \begin{pmatrix} 6y - 6 & 6x \\[1em] 6x & 6y - 6 \end{pmatrix}
    \end{equation*}
    
    \begin{itemize}
        \item For point $(1, 1)$ we get the matrix : $\begin{pmatrix} 0 & 6 \\ 6 & 0\end{pmatrix}$ with eigenvalues $6$ and $-6$. The matrix is not positive or negative definite, so the point is neither local minimum or maximum.
        
        \item For point $(-1, 1)$ we get the matrix : $\begin{pmatrix} 0 & -6 \\ -6 & 0\end{pmatrix}$ with eigenvalues $6$ and $-6$. The matrix is not positive or negative definite, so the point is neither local minimum or maximum.
        
        \item For point $(0, 2)$ we get the matrix : $\begin{pmatrix} 6 & 0 \\ 0 & 6\end{pmatrix}$ with eigenvalues $6$ and $6$. The matrix is positive definite, so the point is local minimum.
        
        \item For point $(0, 0)$ we get the matrix : $\begin{pmatrix} -6 & 0 \\ 0 & -6\end{pmatrix}$ with eigenvalues $-6$ and $-6$. The matrix is negative definite, so the point is local maximum.
    \end{itemize}

    
\end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
You are given the training data $\mathcal{T} = \left \{ ((1, 1)^{\top}, 2), ((1, 2)^{\top}, 3), ((2, 2)^{\top}, 3), ((2, 4)^{\top}, 4) \right \}$.
\begin{enumerate}[a)]
    \item Use linear regression by least squares to compute a predictor for the output. Do this by hand, i.e., don't use the computer and show all computation steps.
    
    We create the matrix $X$ by adding the $1$ before each training data point:
    \begin{equation*}
        X = \begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 2 & 2 \\ 1 & 2 & 4 \end{bmatrix} \quad
        X^{T} = \begin{bmatrix}1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 2 \\ 1 & 2 & 2 & 4  \end{bmatrix}
        \quad
        y = \begin{pmatrix}2 \\ 3 \\ 3 \\ 4  \end{pmatrix}
    \end{equation*}
    
    We compute the matrix $A$ by:
    \begin{equation*}
        A = X^{T} \cdot X = \begin{bmatrix}1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 2 \\ 1 & 2 & 2 & 4  \end{bmatrix} \cdot \begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 2 & 2 \\ 1 & 2 & 4 \end{bmatrix} = \begin{bmatrix} 4 & 6 & 9 \\ 6 & 10 & 15 \\ 9 & 15 & 25 \end{bmatrix}
    \end{equation*}
    
    We compute the vector $b$ by:
    \begin{equation*}
        b = X^{T} \cdot y = \begin{bmatrix}1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 2 \\ 1 & 2 & 2 & 4  \end{bmatrix} \cdot \begin{pmatrix}2 \\ 3 \\ 3 \\ 4  \end{pmatrix} = \begin{pmatrix}12 \\ 19 \\ 30 \end{pmatrix}
    \end{equation*}
    
    Now, we need to solve the equation $A \cdot \beta = b$. We use Cholesky decomposition for express the matrix $A$ in the form $L \cdot L^{T}$, where $L$ is a matrix of the form:
    \begin{equation*}
        L = \begin{bmatrix}a & 0 & 0  \\ b & c & 0 \\ d & e & f  \end{bmatrix}
    \end{equation*}
    
    To compute the entries of the matrix $L$, we use the following formulas:
    \begin{equation*}
        l_{kk} = \sqrt{a_{kk} - \sum_{j = 1}^{k - 1}l_{kj}^2} \quad l_{ki} = \frac{a_{ki} - \sum_{j = 1}^{i - 1} l_{ij} \cdot l_{kj}}{l_{ii}}
    \end{equation*}
    
    \begin{table}[h!]
        \centering
        \begin{tabular}{c c c}
            $l_{11} = \sqrt{a_{11}} = 2 & l_{21} = \frac{a_{21}}{l_{11}} = 3 & l_{22} = \sqrt{a_{22} - l_{21}^{2}} = 1 \\[1em] l_{31} = \frac{a_{31}}{l_{11}} = \frac{9}{2}& l_{32} = \frac{a_{32} - l_{21} \cdot l_{31}}{l_{22}} = \frac{3}{2} & l_{33} = \sqrt{a_{33} - l_{31}^{2} - l_{32}^{2}} = \frac{\sqrt{10}}{2}$
        \end{tabular}
    \end{table}
    
    The matrix $L$ is:
    \begin{equation*}
        L = \begin{bmatrix}2 & 0 & 0  \\ 3 & 1 & 0 \\ \frac{9}{2} & \frac{3}{2} & \frac{\sqrt{10}}{2}  \end{bmatrix}
    \end{equation*}
    
    The equation now becomes $L \cdot L^{T} \cdot \Vec{\beta} = \Vec{b}$. We express $L^{T} \cdot \Vec{\beta} = \Vec{x}$. The final equation becomes:
    \begin{equation*}
        L \cdot \Vec{x} = \Vec{b} \quad \begin{bmatrix}2 & 0 & 0  \\ 3 & 1 & 0 \\ \frac{9}{2} & \frac{3}{2} & \frac{\sqrt{10}}{2}  \end{bmatrix} \cdot \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix} = \begin{pmatrix} 12 \\ 19 \\ 30 \end{pmatrix}
    \end{equation*}
    From the above equation we get:
    \begin{equation*}
        \Vec{x} = \begin{pmatrix} 6 \\ 1 \\ \frac{3 \cdot \sqrt{10}}{10} \end{pmatrix}
    \end{equation*}
    The last equation is:
    \begin{equation*}
        L^{T} \cdot \Vec{\beta} = \Vec{x}
    \end{equation*}
    \begin{equation*}
        \begin{bmatrix}
            2 & 3 & \frac{9}{2} \\
            0 & 1 & \frac{3}{2} \\
            0 & 0 & \frac{\sqrt{10}}{2}
        \end{bmatrix} \cdot \begin{pmatrix} \beta_{1} \\ \beta_{2} \\ \beta_{3} \end{pmatrix} = \begin{pmatrix} 6 \\ 1 \\ \frac{3 \cdot \sqrt{10}}{10} \end{pmatrix}
    \end{equation*}
    From the equation above we get:
    \begin{equation*}
        \Vec{\beta} = \begin{pmatrix}1.5 \\ 0.1 \\ 0.6 \end{pmatrix}
    \end{equation*}
    The linear predictor is:
    \begin{equation*}
        f(x_{1}, x_{2}) = 1.5 + 0.1 \cdot x_{1} + 0.6 \cdot x_{2}
    \end{equation*}
    
    \item Predict the output for $\mathbf{x} = (1.5, 1.5)^{\top}$.
    \begin{equation*}
        f(1.5, 1.5) = 1.5 + 0.1 \cdot 1.5 + 0.6 \cdot 1.5 = 2.55
    \end{equation*}
    
    The predicted value of $(1.5, 1.5)$ is $2.55$.
    
\end{enumerate}

    
\end{homeworkProblem}

\begin{homeworkProblem}
Prove Theorem 3.2 from the lecture. If you consider this as too hard, consult the literature from this field to solve the problem. In case you use external literature, make sure that you give a citation for that literature (i.e. say where you took it from) and rephrase the proof appropriately.\\

Assuming the input, output and mixed joint density from Theorem 3.2. To find the function $f : R^{D} \to R_{G}$ that minimizes the expected prediction error EPE($f$) with respect to the 0 - 1 loss, the following steps can be taken:
\begin{equation*}
    \begin{split}
        \argmin_{g \in R_{G}} \text{EPE}(f(\textbf{x})) & = \argmin_{g \in R_{G}} E[L(g, f(\textbf{x})) | \textbf{X} = \textbf{x}] \\
        & = \argmin_{g \in R_{G}} P(g = g_{1} | \textbf{X} = \textbf{x}) \cdot L(g_{1}, f(\textbf{x})) + \ldots + P(g = g_{r} | \textbf{X} = \textbf{x}) \cdot L(g_{r}, f(\textbf{x})) \\
        & = \argmin_{g \in R_{G}} \sum_{i = 1}^{r} P(g = g_{i} | \textbf{X} = \textbf{x}) \cdot L(g_{i}, f(\textbf{x})
    \end{split}
\end{equation*}

Due to the loss function having a range of 0 and 1, the last result can be simplified to a sum of the probabilities for which the predicted class, $f(x)$, does not equal the true class, $g$.
\begin{equation*}
    \begin{split}
        \argmin_{g \in R_{G}} \text{EPE}(f(\textbf{x})) & = \argmin_{g \in R_{G}} P(g \neq f(\textbf{x}) | \textbf{X} = \textbf{x}) \\
        & = \argmin_{g \in R_{G}} 1 - P(g = f(\textbf{x}) | \textbf{X} = \textbf{x}) \\
        & = \argmax_{g \in R_{G}} P(g = f(\textbf{x}) | \textbf{X} = \textbf{x})
    \end{split}
\end{equation*}

If $\mathfrak{g}$ is the true output class, such that $p(\mathfrak{g} | \textbf{x}) = \max_{g \in R_{G}} p(g | \textbf{x})$, then the minimized $f(\textbf{x})$ found above matches the definition of the Bayes classifier.

\end{homeworkProblem}

\begin{programmingProblem}
Consider the Examples 4.2 and 4.3 from the lecture, for which you also have access to the source code. Complete the following tasks:
\begin{enumerate}[a)]
    \item (Re-)implement Example 4.3. This time, however you need to implement the linear regression yourself, without a machine learning library. (If you implement in Python, just start from the available Jupyter notebook.) Verify the correctness of your implementation by cross-checking it with Example 4.3.
    \item Apply your implementation to the Energy efficiency Data Set from the UCI Machine Learning Repository. Build the predictor for the required heating load on the full data set and predict the load on the first three samples of the data set.
    \item Implement Example 4.5 starting from Example 4.2 from the lecture notes.
\end{enumerate}

The implementation of the linear regressor by least squares can be found in the file \texttt{programming\_exercises.ipynb}. The class offers two methods, \texttt{fit} and \texttt{predict}, which add the data and train the model, and predict the expected regression value.
\end{programmingProblem}

\end{document}
