\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}



\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{May 2, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{11}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
In this paper \& pencil task, you compare the quadratic model based on the basis functions $\{ 1, X, X^{2} \}$ to the linear model for the following training set.
\begin{equation*}
    \mathcal{T} = \{ (-1, -1), (0, -4), (2, 2) \}
\end{equation*}
\begin{enumerate}[a)]
    \item Compute the linear model using least squares for the given training data.
    
    Firstly, we create the matrix $\mathcal{X}$:
    
    \begin{equation*}
        \mathcal{X} = \begin{pmatrix}1 & -1 \\ 1 & 0 \\ 1 & 2 \end{pmatrix} \hspace{1cm}
        \mathcal{X}^\top = \begin{pmatrix}1 & 1 & 1 \\ -1 & 0 & 2 \end{pmatrix} \hspace{1cm}
        y = \begin{pmatrix}-1 \\ 4 \\ 2\end{pmatrix}
    \end{equation*}

    
    By using the least squares, we compute the coefficients of the model:
    \begin{equation*}
        \alpha = (\mathcal{X}^{\top} \cdot \mathcal{X})^{-1} \cdot \mathcal{X}^\top \cdot y
    \end{equation*}
    
    \begin{equation*}
        \alpha = \Bigg(\begin{pmatrix}1 & 1 & 1 \\ -1 & 0 & 2 \end{pmatrix} \cdot \begin{pmatrix}1 & -1 \\ 1 & 0 \\ 1 & 2 \end{pmatrix}\Bigg)^{-1} \cdot \begin{pmatrix}1 & 1 & 1 \\ -1 & 0 & 2 \end{pmatrix} \cdot \begin{pmatrix}-1 \\ 4 \\ 2\end{pmatrix} = \begin{pmatrix} -1.4285714285714286 \\ 1.28571429 \end{pmatrix}
    \end{equation*}
    
    So the derived model is:
    \begin{equation*}
        f_{\alpha}(\mathbf{X}) =  -1.4285714285714286 + 1.28571429 \cdot \mathbf{X}
    \end{equation*}
    \item Compute the quadratic model using least squares for the given training data.
    
    Firstly, we create the matrix $\mathcal{X}$:
    
    \begin{equation*}
        \mathcal{X} = \begin{pmatrix}1 & -1 & 1 \\ 1 & 0 & 0\\ 1 & 2 & 4\end{pmatrix} \hspace{1cm}
        \mathcal{X}^\top = \begin{pmatrix}1 & 1 & 1 \\ -1 & 0 & 2\\ 1 & 0 & 4\end{pmatrix} \hspace{1cm}
        y = \begin{pmatrix}-1 \\ 4 \\ 2\end{pmatrix}
    \end{equation*}
    
    By using the least squares, we compute the coefficients of the model:
    \begin{equation*}
        \alpha = (\mathcal{X}^{\top} \cdot \mathcal{X})^{-1} \cdot \mathcal{X}^\top \cdot y
    \end{equation*}
    
    \begin{equation*}
        \alpha = \Bigg( \begin{pmatrix}1 & 1 & 1 \\ -1 & 0 & 2\\ 1 & 0 & 4\end{pmatrix} \cdot \begin{pmatrix}1 & -1 & 1 \\ 1 & 0 & 0\\ 1 & 2 & 4\end{pmatrix} \Bigg) \cdot \begin{pmatrix}1 & 1 & 1 \\ -1 & 0 & 2\\ 1 & 0 & 4\end{pmatrix} \cdot \begin{pmatrix}-1 \\ 4 \\ 2\end{pmatrix} = \begin{pmatrix} -4 \\ -1 \\  2 \end{pmatrix}
    \end{equation*}
    
    So, the derived model is:
    
    \begin{equation*}
          f_{\alpha}(\mathbf{X}) = -4 -\mathbf{X} + 2 \cdot \mathbf{X}^{2}
    \end{equation*}
    
    \pagebreak
    
    \item Draw (by hand) the linear and the quadratic model in a plot, together with the training samples.
    
    The graph of linear and quadratic model is represented as following:
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=\textwidth]{p01.png}
        \caption{}
        \label{fig:my_label}
    \end{figure}
    \pagebreak
\end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
You are given the training data
\begin{equation*}
    \mathcal{T} = \{ (-2, 2)^{\top}, (0, 0)^{\top}, (1, 2)^{\top} \}
\end{equation*}
\begin{enumerate}[a)]
    \item Use the Gaussian kernel with kernel width $\sigma = 1$ and compute by hand the kernel-based model using least squares for the given training data.
    
    Firstly, we calculate the Gaussian kernel for each of the input points:
    
    \begin{equation*}
        k(-2, 0) = 0.0183156389 \hspace{2cm} k(-2, 1) = 0.000123409804 \hspace{2cm} k(0, 1) = 0.367879441
    \end{equation*}
    
    The Gaussian kernel of a point with itself is $1$ since the distance between a point and itself is $0$. The Gaussian kernel is also symmetric, so the kernel of the opposite points related to above ones will be the same. The Kernel matrix is as following:
    
    \begin{equation*}
        \mathcal{A} = \begin{pmatrix}1 &  0.0183156389 & 0.000123409804 \\ 0.0183156389 & 1 & 0.367879441 \\  0.000123409804 & 0.367879441 & 1\end{pmatrix}
    \end{equation*}
    
    Now we calculate the coefficient vector by using the formula:
    
    \begin{equation*}
        \alpha = \mathcal{A}^{-1} \cdot y
    \end{equation*}
    
    \begin{equation*}
        \alpha = \begin{pmatrix}1 &  0.0183156389 & 0.000123409804 \\ 0.0183156389 & 1 & 0.367879441 \\  0.000123409804 & 0.367879441 & 1\end{pmatrix}^{-1} \cdot \begin{pmatrix} 2 \\ 0 \\ 2 \end{pmatrix} = \begin{pmatrix}2.01607799 \\ -0.89351756 \\ 2.32845794 \end{pmatrix}
    \end{equation*}
    
    The derived model is:
    \begin{equation*}
        f_{\alpha}(\mathbf{X}) = 2.01607799 \cdot k(X, -2) - 0.89351756 \cdot k(X, 0) + 2.32845794 \cdot k(X, 1)
    \end{equation*}
    \item Repeat the previous task, but this time you compute by hand the kernel-based model using ridge regression with regularization parameter $\lambda = 1$.
    
    We calculate the coefficient vector by the formula:
    
    \begin{equation*}
        \alpha = (\mathcal{A} + \lambda \cdot \mathcal{I})^{-1} \cdot y
    \end{equation*}
    
    \begin{equation*}
        \alpha = \Bigg(\begin{pmatrix}1 &  0.0183156389 & 0.000123409804 \\ 0.0183156389 & 1 & 0.367879441 \\  0.000123409804 & 0.367879441 & 1\end{pmatrix} + \begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0\\ 0 & 0 & 1 \end{pmatrix}\Bigg)^{-1} \cdot \begin{pmatrix} 2 \\ 0 \\ 2\end{pmatrix} = \begin{pmatrix} 1.00176635 \\ -0.19986453 \\  1.03670121 \end{pmatrix}
    \end{equation*}
    
    The derived model is:
    
    \begin{equation*}
        f_{\alpha}(\mathbf{X}) = 1.00176635 \cdot k(X, -1) - 0.19986453 \cdot k(X, 0) + 1.03670121 \cdot k(X, 1)
    \end{equation*}

    \pagebreak
    
    \item Draw both above models including the training data in one plot.
    
    The graphs of both models are represented as following:
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=\textwidth]{p02.png}
        \caption{}
        \label{fig:my_label}
    \end{figure}
\end{enumerate}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
Prove Lemma 10.2 from the lecture notes.
\end{homeworkProblem}

\begin{programmingProblem}
In this task, we implement kernel ridge regression. To this end, we start from Example 10.6 from the lecture notes, which is available as Jupyter notebook. However, instead of using scikit-learn to compute the ridge regression model, you replace that part of the code by your implementation of kernel ridge regression, which follows Algorithm 17 from the lecture notes. Due to the existing implementation, it will be easier to evaluate, whether your implementation is correct.
\end{programmingProblem}
\end{document}
