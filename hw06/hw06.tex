\documentclass{article}

\usepackage{extramarks}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage{hyperref}

\usepackage{tikz}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[shortlabels]{enumitem}

\usepackage{float,graphicx}

\usepackage{pgfplots}

\usepackage{adjustbox}


\usepackage{array}
\usepackage{pgfgantt}

\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}
\usepackage{fancybox}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Listings' Styles

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}


\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=python}
\lstset{language=Python}

\linespread{1.1}
\captionsetup[table]{position=bottom}

\pagestyle{fancy}
\lhead{\groupName}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\today}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand\qedsymbol{$\blacksquare$}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Assignment Sheet\ \#\hmwkNumber}
\newcommand{\hmwkDueDate}{March 22, 2022}
\newcommand{\hmwkClass}{Machine Learning}
\newcommand{\hmwkAuthorName}{Henri Sota, Enis Mustafaj}
\newcommand{\groupName}{\textbf{Group HB}}

% Homework Number Variable
\newcommand{\hmwkNumber}{6}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \stepcounter{#1}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}

\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \hmwkNumber \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}


\newcounter{programmingPartCounter}
\newcounter{programmingProblemCounter}

\setcounter{programmingProblemCounter}{1}
\nobreak\extramarks{Programming Problem \hmwkNumber \arabic{programmingProblemCounter}}{}\nobreak{}

%
% Programming Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{programmingProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{programmingProblemCounter}{\hmwkNumber.#1}
    \fi
    \section{Programming Problem \hmwkNumber.\arabic{programmingProblemCounter}}
    \setcounter{programmingPartCounter}{1}
    \enterProblemHeader{programmingProblemCounter}
}{
    \exitProblemHeader{programmingProblemCounter}
}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 10:00}\\
    \vspace{3in}
}

\author{\groupName \\ \hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\comment}[1]{} % Multi-line comment

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
Consider the following training data:
\begin{equation*}
    \mathcal{T} = \left \{ ((-1, 0)^{\top}, 3), ((1, 3)^{\top}, -1), ((-2, 1)^{\top}, 0), ((0, 4)^{\top}, 2) \right \}
\end{equation*}

We want to compute the predictor from linear regression by least squares. However, instead of using Theorem 4.1, we use batch gradient descent to compute the coefficient vector $\hat{\beta}$. Choose the initial guess $\beta^{(0)} = (0, 0, 1)^{\top}$ and calculate the first two steps of batch gradient descent with learning rate $\eta = 0.25$.

    To calculate the coefficients of each step, we use the formula for the batch gradient descent
    
    \begin{equation*}
        \beta^{(n + 1)} = \beta^{(n)} -  \eta \cdot \sum_{i=1}^{N}\nabla L(y_{i}, f(x_{i}))
    \end{equation*}
    
    First, we calculate the partial derivatives of the loss function with respect to coefficient vector:
    
    \begin{equation*}
        \nabla L(y_{i}, f(x_{i})) = 
        \begin{pmatrix}
            \frac{\partial L(y, f(x))}{\beta_{0}} \\[0.5em] \frac{\partial L(y, f(x))}{\beta_{1}} \\[0.5em] \frac{\partial L(y, f(x))}{\beta_{2}}
        \end{pmatrix} =
        \begin{pmatrix}
            -2 \cdot (y - \beta_{0} - \beta_{1} \cdot x_{0} + \beta_{2} \cdot x_{2}) \\
            -2 \cdot (y - \beta_{0} - \beta_{1} \cdot x_{0} + \beta_{2} \cdot x_{2}) \cdot x_{1} \\
            -2 \cdot (y - \beta_{0} - \beta_{1} \cdot x_{0} + \beta_{2} \cdot x_{2}) \cdot x_{2}
        \end{pmatrix}
    \end{equation*}
    
    By applying this to the general formula, we get:
    \begin{equation*}
         \beta^{(n + 1)} = \beta^{(n)} + 2 \cdot \eta \cdot \sum_{i=1}^{N} \begin{pmatrix}
            (y_{i} - \beta_{0} - \beta_{1} \cdot x_{0(i)} - \beta_{2} \cdot x_{2(i)}) \\
            (y_{i} - \beta_{0} - \beta_{1} \cdot x_{0(i)} - \beta_{2} \cdot x_{2(i)}) \cdot x_{1(i)} \\
            (y_{i} - \beta_{0} - \beta_{1} \cdot x_{0(i)} - \beta_{2} \cdot x_{2(i)}) \cdot x_{2(i)}
        \end{pmatrix}
    \end{equation*}
    
    We substitute the values of training data in the formula to compute the coefficients from the first 2 iteration steps.
    \begin{equation*}
        \beta^{1} = \beta^{0} + \frac{1}{2} \cdot \left [ \begin{pmatrix} 3 \\ -3 \\ 0 \end{pmatrix} + \begin{pmatrix} -4 \\ -4 \\ -12 \end{pmatrix} + \begin{pmatrix} -1 \\ 2 \\ -1 \end{pmatrix} + \begin{pmatrix} -6 \\ 0 \\ -24 \end{pmatrix} \right ]
    \end{equation*}
    
    \begin{equation*}
        \beta^{1} = \begin{pmatrix}
            0 \\ 0 \\ -1
        \end{pmatrix} + \frac{1}{2} \cdot \begin{pmatrix}
            -8 \\ -5 \\ -37
        \end{pmatrix} = 
        \begin{pmatrix}
            -4 \\ -\frac{5}{2} \\[0.5em] -\frac{35}{2}
        \end{pmatrix}
    \end{equation*}
    
    To compute the second step we use the values of the new coefficients derived from above.
    
    \begin{equation*}
        \beta^{2} = \beta^{1} + \frac{1}{2} \cdot \left [ \begin{pmatrix} \frac{9}{2} \\ -\frac{9}{2} \\ 0 \end{pmatrix} + \begin{pmatrix} 58 \\ 28 \\ 174 \end{pmatrix} + \begin{pmatrix} \frac{33}{2} \\ -33 \\ \frac{33}{2} \end{pmatrix} + \begin{pmatrix} 76 \\ 0 \\ 304 \end{pmatrix} \right ]
    \end{equation*}
    
     \begin{equation*}
        \beta^{2} = \begin{pmatrix}
            -4 \\[0.5em] \frac{-5}{2} \\[0.5em] \frac{-35}{2} 
        \end{pmatrix} + \frac{1}{2} \cdot \begin{pmatrix}
            155 \\[0.5em] \frac{41}{2} \\[0.5em] \frac{849}{2}
        \end{pmatrix} = 
        \begin{pmatrix}
            \frac{147}{2} \\[0.5em] \frac{31}{2} \\[0.5em] \frac{919}{4}
        \end{pmatrix}   
    \end{equation*}

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
In this task, we consider again the training data
\begin{equation*}
    \mathcal{T} = \left \{ ((4, 1)^{\top}, 2), ((2, 8)^{\top}, -14), ((1, 0)^{\top}, 1), ((3, 2)^{\top}, -1) \right \}
\end{equation*}

However, this time we do not use the linear model. Instead we use the quadratic model
\begin{equation*}
    f(\textbf{X}) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \beta_{3}X_{1}X_{2} + \beta_{4}X_{1}^{2} \beta_{5}X_{2}^{2}
\end{equation*}
and stick to the $L_{2}$ loss.
\begin{enumerate}[a)]
    \item Derive the gradient $\nabla_{\beta} L_{2}(y_{i}, f(\mathbf{x_{i}}))$ of the loss with respect to the coefficient vector.
    
    \begin{equation*}
        \resizebox{0.95\textwidth}{!}{$
        \nabla_{\beta} L_{2}(y_{i}, f(\mathbf{x_{i}})) = 
        \begin{pmatrix}
            \frac{\partial L_{2}(y_{i}, f(\mathbf{x_{i}}))}{\beta_{0}} \\[0.5em]
            \frac{\partial L_{2}(y_{i}, f(\mathbf{x_{i}}))}{\beta_{1}} \\[0.5em]
            \frac{\partial L_{2}(y_{i}, f(\mathbf{x_{i}}))}{\beta_{2}} \\[0.5em]
            \frac{\partial L_{2}(y_{i}, f(\mathbf{x_{i}}))}{\beta_{3}} \\[0.5em]
            \frac{\partial L_{2}(y_{i}, f(\mathbf{x_{i}}))}{\beta_{4}} \\[0.5em]
            \frac{\partial L_{2}(y_{i}, f(\mathbf{x_{i}}))}{\beta_{5}} 
        \end{pmatrix} = 
        \begin{pmatrix}
            -2 \cdot (y_{1} - (\beta_{0} + \beta_{1}\cdot x_{1} + \beta_{2}\cdot x_{2} + \beta_{3}\cdot x_{1} \cdot x_{2} + \beta_{4} \cdot x_{1}^{2} + \beta_{5} \cdot x_{2}^{2})) \\
            -2 \cdot (y_{1} - (\beta_{0} + \beta_{1}\cdot x_{1} + \beta_{2}\cdot x_{2} + \beta_{3}\cdot x_{1} \cdot x_{2} + \beta_{4} \cdot x_{1}^{2} + \beta_{5} \cdot x_{2}^{2})) \cdot x_{1} \\
            -2 \cdot (y_{1} - (\beta_{0} + \beta_{1}\cdot x_{1} + \beta_{2}\cdot x_{2} + \beta_{3}\cdot x_{1} \cdot x_{2} + \beta_{4} \cdot x_{1}^{2} + \beta_{5} \cdot x_{2}^{2})) \cdot x_{2} \\
            -2 \cdot (y_{1} - (\beta_{0} + \beta_{1}\cdot x_{1} + \beta_{2}\cdot x_{2} + \beta_{3}\cdot x_{1} \cdot x_{2} + \beta_{4} \cdot x_{1}^{2} + \beta_{5} \cdot x_{2}^{2})) \cdot x_{1} x_{2} \\
            -2 \cdot (y_{1} - (\beta_{0} + \beta_{1}\cdot x_{1} + \beta_{2}\cdot x_{2} + \beta_{3}\cdot x_{1} \cdot x_{2} + \beta_{4} \cdot x_{1}^{2} + \beta_{5} \cdot x_{2}^{2})) \cdot x_{1}^{2} \\
            -2 \cdot (y_{1} - (\beta_{0} + \beta_{1}\cdot x_{1} + \beta_{2}\cdot x_{2} + \beta_{3}\cdot x_{1} \cdot x_{2} + \beta_{4} \cdot x_{1}^{2} + \beta_{5} \cdot x_{2}^{2})) \cdot x_{2}^{2}
        \end{pmatrix}$
        }
    \end{equation*}
    \item Use the initial guess $\beta^{(0)} = \mathbf{0}$ and the learning rate $\eta = 0.1$ and perform the first two steps of stochastic gradient descent. Recall that in stochastic gradient descent, we at some point pick random samples from the training set. In order to get a unique solution in this task, we assume that a random selection of samples from the training set would give you the training samples in their original order.
    
    To calculate the coefficients we use the formula:
    \begin{equation*}
        \beta^{(n + 1)} = \beta^{(n)} + 2 \cdot \eta \cdot X^{T} \cdot (y - X \cdot \beta^{n})
    \end{equation*}
    For the first step, we calculate:
    
    \begin{equation*}
        X = \begin{pmatrix}1 & 4 & 1 & 4 & 16 & 1 \end{pmatrix} \hspace{1cm} X^{T} = \begin{pmatrix}
            1 \\ 4 \\ 1 \\ 4 \\ 16 \\ 1
        \end{pmatrix} \hspace{1cm}
        y = 2
    \end{equation*}
    
    \begin{equation*}
        \beta^{1} = \begin{pmatrix}
            0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
        \end{pmatrix} + \frac{1}{5} \cdot \begin{pmatrix}
            1 \\ 4 \\ 1 \\ 4 \\ 16 \\ 1
        \end{pmatrix} \cdot \left (2 - \begin{bmatrix}1 & 4 & 1 & 4 & 16 & 1 \end{bmatrix} \cdot \begin{pmatrix}
            0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
        \end{pmatrix} \right ) = \begin{pmatrix}
            \frac{2}{5} \\[0.5em] \frac{8}{5} \\[0.5em] \frac{2}{5} \\[0.5em] \frac{8}{5} \\[0.5em] \frac{32}{5} \\[0.5em] \frac{2}{5}
        \end{pmatrix}
    \end{equation*}
    
    For the second step, we calculate:
    \begin{equation*}
        X = \begin{pmatrix}1 & 2 & 8 & 16 & 4 & 64 \end{pmatrix} \hspace{1cm} X^{T} = \begin{pmatrix}
            1 \\ 2 \\ 8 \\ 16 \\ 4 \\ 64
        \end{pmatrix} \hspace{1cm}
        y =  -14 
    \end{equation*}
    \begin{equation*}
        \beta^{2} = \begin{pmatrix}
            \frac{2}{5} \\[0.5em] \frac{8}{5} \\[0.5em] \frac{2}{5} \\[0.5em] \frac{8}{5} \\[0.5em] \frac{32}{5} \\[0.5em] \frac{2}{5}
        \end{pmatrix} + \frac{1}{5} \cdot \begin{pmatrix}
            1 \\ 2 \\ 8 \\ 16 \\ 4 \\ 64
        \end{pmatrix} \cdot \left ( -14 - \begin{pmatrix}1 & 2 & 8 & 16 & 4 & 64 \end{pmatrix} \cdot \begin{pmatrix}
            \frac{2}{5} \\[0.5em] \frac{8}{5} \\[0.5em] \frac{2}{5} \\[0.5em] \frac{8}{5} \\[0.5em] \frac{32}{5} \\[0.5em] \frac{2}{5}
        \end{pmatrix} \right ) = \begin{pmatrix}-\frac{478}{25} \\[0.5em] -\frac{936}{25} \\[0.5em] -\frac{3894}{25} \\[0.5em] -\frac{7768}{25} \\[0.5em] -\frac{1792}{25} \\[0.5em] -\frac{31222}{25}\end{pmatrix}
    \end{equation*}
    \item Use the initial guess $\beta^{(0)} = \mathbf{0}$ and the learning rate $\eta = 0.1$ and perform the first two steps of mini-batch gradient descent with a batch size of $N_{b} = 2$. Use the same approach for the randomization part as in the previous sub-task.
    
    For the first step we have:
    \begin{equation*}
        X = \begin{pmatrix}1 & 4 & 1 & 4 & 16 & 1 \\ 1 & 2 & 8 & 16 & 4 & 64  \end{pmatrix}
        \hspace{1cm}
        X^{T} = \begin{pmatrix}
            1 & 1 \\
            4 & 2 \\
            1 & 8 \\
            4 & 16 \\
            16 & 4 \\
            1 & 64
        \end{pmatrix}
        \hspace{ 1cm}
        y = \begin{pmatrix}
            2 \\ -14
        \end{pmatrix}
    \end{equation*}
    
    Now we calculate the coefficient vector for the first step:
    \begin{equation*}
        \beta^{1} = \beta^{0} + 2 \cdot \eta \cdot X^{T} \cdot (y - X \cdot \beta^{0})
    \end{equation*}
    
    \begin{equation*}
        \beta{1} = \begin{pmatrix}
            0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
        \end{pmatrix} + \frac{1}{5} \cdot \begin{pmatrix}
            1 & 1 \\
            4 & 2 \\
            1 & 8 \\
            4 & 16 \\
            16 & 4 \\
            1 & 64
        \end{pmatrix} \cdot \left ( \begin{pmatrix}
            2 \\ -14
        \end{pmatrix} - \begin{pmatrix}1 & 4 & 1 & 4 & 16 & 1 \\ 1 & 2 & 8 & 16 & 4 & 64  \end{pmatrix} \cdot \begin{pmatrix}
            0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
        \end{pmatrix} \right )
    \end{equation*}
    \begin{equation*}
        \beta^{1} = \begin{pmatrix}
            -\frac{12}{5} \\[0.5em] -4 \\[0.5em] -22 \\[0.5em] -\frac{216}{5} \\[0.5em] -\frac{24}{5} \\[0.5em] -\frac{894}{5}
        \end{pmatrix}
    \end{equation*}
    For the second step we have:
    \begin{equation*}
        X = \begin{pmatrix}1 & 1 & 0 & 0 & 1 & 0 \\ 1 & 3 & 2 & 6 & 9 & 4  \end{pmatrix}
        \hspace{1cm}
        X^{T} = \begin{pmatrix}
            1 & 1 \\
            1 & 3 \\
            0 & 2 \\
            0 & 6 \\
            1 & 9 \\
            0 & 4
        \end{pmatrix}
        \hspace{ 1cm}
        y = \begin{pmatrix}
            1 \\ -1
        \end{pmatrix}
    \end{equation*}
    Now we calculate the coefficient vector for the second step:
    \begin{equation*}
        \beta^{2} = \beta^{1} + 2 \cdot \eta \cdot X^{T} \cdot (y - X \cdot \beta^{1})
    \end{equation*}
    \begin{equation*}
        \beta{2} = \begin{pmatrix}
            -\frac{12}{5} \\[0.5em] -4 \\[0.5em] -22 \\[0.5em] -\frac{216}{5} \\[0.5em] -\frac{24}{5} \\[0.5em] -\frac{894}{5}
        \end{pmatrix} + \frac{1}{5} \cdot \begin{pmatrix}
            1 & 1 \\
            1 & 3 \\
            0 & 2 \\
            0 & 6 \\
            1 & 9 \\
            0 & 4
        \end{pmatrix} \cdot \left ( \begin{pmatrix}
            1 \\ -11
        \end{pmatrix} - \begin{pmatrix}1 & 1 & 0 & 0 & 1 & 0 \\ 1 & 3 & 2 & 6 & 9 & 4  \end{pmatrix} \cdot \begin{pmatrix}
            -\frac{12}{5} \\[0.5em] -4 \\[0.5em] -22 \\[0.5em] -\frac{216}{5} \\[0.5em] -\frac{24}{5} \\[0.5em] -\frac{894}{5}
        \end{pmatrix} \right )
    \end{equation*}
    
    \begin{equation*}
        \beta{2} = \begin{pmatrix}
            -\frac{12}{5} \\[0.5em] -4 \\[0.5em] -22 \\[0.5em] -\frac{216}{5} \\[0.5em] -\frac{24}{5} \\[0.5em] -\frac{894}{5}
        \end{pmatrix} + \frac{1}{5} \cdot \begin{pmatrix}
            1 & 1 \\
            1 & 3 \\
            0 & 2 \\
            0 & 6 \\
            1 & 9 \\
            0 & 4
        \end{pmatrix} \cdot \left ( \begin{pmatrix}
            1 \\[0.5em] -11
        \end{pmatrix} + \begin{pmatrix}\frac{56}{5}\\[0.5em] \frac{5380}{5}\end{pmatrix} \right )
    \end{equation*}
    
    \begin{equation*}
        \beta^{2} = \begin{pmatrix}-\frac{12}{5} \\[0.5em] -4 \\[0.5em] -22 \\[0.5em] \frac{216}{5} \\[0.5em] -\frac{24}{5} \\[0.5em] -\frac{894}{5}\end{pmatrix}+\begin{pmatrix}\frac{5436}{25} \\[0.5em] \frac{16186}{25} \\[0.5em] 430 \\[0.5em] 1290 \\[0.5em] \frac{48436}{25} \\[0.5em] 860\end{pmatrix} = \begin{pmatrix}\frac{5376}{25} \\[0.5em] \frac{16086}{25} \\[0.5em] 408\\ \frac{6234}{5} \\[0.5em] \frac{48316}{25} \\[0.5em] \frac{3406}{5}\end{pmatrix}
    \end{equation*}
    
\end{enumerate}
\end{homeworkProblem}
\pagebreak
\begin{programmingProblem}
In this programming exercise, you will implement mini-batch gradient descent in order to train a linear model using least squares. Note that you can use an implementation of mini-batch gradient descent to get both, batch gradient descent and stochastic gradient descent.

As training data, you will use again the \href{https://archive.ics.uci.edu/ml/datasets/Energy+efficiency}{Energy efficiency Data Set} from the UCI Machine Learning Repository, where we consider the required heating energy as output quantity.

To be more specific, you are supposed to reproduce Example 5.2 from the lecture notes, however with your own implementation of mini-batch gradient descent.\\

The implementation of gradient descent can be found in the file \texttt{programming\_exercises.ipynb}. There are three classes, \texttt{MiniBatchGradientDescentLinearRegressor}, \texttt{SGDLinearRegressor}, and \\\texttt{BGDLinearRegressor}, the latter two being simply proxies for the first one. The first class implements the algorithm for gradient descent in the \texttt{fit} method. The class expects the training data to be NumPy arrays. The latter classes expect only two arguments, \texttt{learning\_rate} and \texttt{epochs}, while the parent class also expects the \texttt{batch\_size} argument. In case \texttt{batch\_size} argument is not provided, it is set to match the whole training set size given. Example 5.2 is also provided at the end of the notebook for comparison.
\end{programmingProblem}

\end{document}
